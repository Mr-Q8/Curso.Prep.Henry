{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr-Q8/Curso.Prep.Henry/blob/master/LTX_Video_Tx_to_Vid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LTX-VIDEO Text to Video**"
      ],
      "metadata": {
        "id": "f4p1ysFKMbs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- You can use the free T4 GPU to run this depending on the output video resolution and number of frames. The default setting runs without issues, but at 768 by 512 output resolution with 121 frames, the decoding process crashes the 12.7GB RAM.  For faster video generation with higher resolutions and frames, use higher GPUs.\n",
        "- If you want to generate a video with n frames, then set frames to n+1. e.g. To generate a video with 72 frames, set frames to 73.\n",
        "- You need to use detailed prompts to get decent results.\n",
        "- Videos are generated at 24fps."
      ],
      "metadata": {
        "id": "EBB00lC6q-DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prepare Environment\n",
        "!pip install torch==2.6.0 torchvision==0.21.0\n",
        "%cd /content\n",
        "Always_Load_Models_for_Inference = False\n",
        "Use_t5xxl_fp16 = False\n",
        "# Install dependencies\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.29.post2\n",
        "!pip install av\n",
        "!git clone https://github.com/Isi-dev/ComfyUI\n",
        "%cd /content/ComfyUI\n",
        "!apt -y install -qq aria2 ffmpeg\n",
        "\n",
        "# Download required models\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors -d /content/ComfyUI/models/checkpoints -o ltx-video-2b-v0.9.5.safetensors\n",
        "if Use_t5xxl_fp16:\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp16.safetensors -d /content/ComfyUI/models/text_encoders -o t5xxl_fp16.safetensors\n",
        "else:\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors -d /content/ComfyUI/models/text_encoders -o t5xxl_fp8_e4m3fn_scaled.safetensors\n",
        "\n",
        "# Initial setup\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gc\n",
        "import sys\n",
        "import random\n",
        "import os\n",
        "import imageio\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from comfy import model_management\n",
        "\n",
        "from nodes import (\n",
        "    CheckpointLoaderSimple,\n",
        "    CLIPLoader,\n",
        "    CLIPTextEncode,\n",
        "    VAEDecode\n",
        ")\n",
        "\n",
        "from comfy_extras.nodes_custom_sampler import (\n",
        "    KSamplerSelect,\n",
        "    SamplerCustom\n",
        ")\n",
        "\n",
        "from comfy_extras.nodes_lt import (\n",
        "    LTXVConditioning,\n",
        "    LTXVScheduler,\n",
        "    EmptyLTXVLatentVideo\n",
        ")\n",
        "\n",
        "checkpoint_loader = CheckpointLoaderSimple()\n",
        "clip_loader = CLIPLoader()\n",
        "clip_encode_positive = CLIPTextEncode()\n",
        "clip_encode_negative = CLIPTextEncode()\n",
        "scheduler = LTXVScheduler()\n",
        "sampler_select = KSamplerSelect()\n",
        "conditioning = LTXVConditioning()\n",
        "empty_latent_video = EmptyLTXVLatentVideo()\n",
        "sampler = SamplerCustom()\n",
        "vae_decode = VAEDecode()\n",
        "\n",
        "# if not Always_Load_Models_for_Inference:\n",
        "# with torch.inference_mode():\n",
        "#     # Load models\n",
        "#     print(\"Loading Model...\")\n",
        "#     model, _, vae = checkpoint_loader.load_checkpoint(\"ltx-video-2b-v0.9.5.safetensors\")\n",
        "#     print(\"Loaded model!\")\n",
        "#     # print(\"Loading Text_Encoder...\")\n",
        "#     # clip = clip_loader.load_clip(\"t5xxl_fp8_e4m3fn_scaled.safetensors\", \"ltxv\", \"default\")[0]\n",
        "#     # print(\"Loaded Text_Encoder!\")\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Frees GPU (VRAM) and CPU RAM memory.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "def generate_video(\n",
        "    positive_prompt: str = \"A drone quickly rises through a bank of morning fog...\",\n",
        "    negative_prompt: str = \"low quality, worst quality...\",\n",
        "    width: int = 768,\n",
        "    height: int = 512,\n",
        "    seed: int = 0,\n",
        "    steps: int = 30,\n",
        "    cfg_scale: float = 2.05,\n",
        "    sampler_name: str = \"res_multistep\",\n",
        "    length: int = 49,\n",
        "    fps: int = 24\n",
        "):\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        print(\"Loading Text_Encoder...\")\n",
        "        clip = clip_loader.load_clip(\"t5xxl_fp8_e4m3fn_scaled.safetensors\", \"ltxv\", \"default\")[0]\n",
        "        print(\"Loaded Text_Encoder!\")\n",
        "\n",
        "    try:\n",
        "        assert width % 32 == 0, \"Width must be divisible by 32\"\n",
        "        assert height % 32 == 0, \"Height must be divisible by 32\"\n",
        "\n",
        "        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n",
        "        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"Text_Encoder removed from memory\")\n",
        "\n",
        "        empty_latent = empty_latent_video.generate(width, height, length)[0]\n",
        "\n",
        "        sigmas = scheduler.get_sigmas(steps, cfg_scale, 0.95, True, 0.1)[0]\n",
        "        selected_sampler = sampler_select.get_sampler(sampler_name)[0]\n",
        "        conditioned = conditioning.append(positive, negative, 25.0)\n",
        "\n",
        "        print(\"Loading model & VAE...\")\n",
        "        model, _, vae = checkpoint_loader.load_checkpoint(\"ltx-video-2b-v0.9.5.safetensors\")\n",
        "        print(\"Loaded model & VAE!\")\n",
        "\n",
        "        print(\"Generating video...\")\n",
        "        sampled = sampler.sample(\n",
        "            model=model,\n",
        "            add_noise=True,\n",
        "            noise_seed=seed if seed != 0 else random.randint(0, 2**32),\n",
        "            cfg=cfg_scale,\n",
        "            positive=conditioned[0],\n",
        "            negative=conditioned[1],\n",
        "            sampler=selected_sampler,\n",
        "            sigmas=sigmas,\n",
        "            latent_image=empty_latent\n",
        "        )[0]\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"Model removed from memory\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "          try:\n",
        "              print(\"Decodimg Latents...\")\n",
        "              decoded = vae_decode.decode(vae, sampled)[0].detach()\n",
        "              print(\"Latents Decoded!\")\n",
        "              del vae\n",
        "              torch.cuda.empty_cache()\n",
        "              gc.collect()\n",
        "              print(\"VAE removed from memory\")\n",
        "\n",
        "              output_path = \"/content/output.mp4\"\n",
        "              with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "                  for i, frame in enumerate(decoded):\n",
        "                      frame_np = (frame.cpu().numpy() * 255).astype(np.uint8)\n",
        "                      writer.append_data(frame_np)\n",
        "                      if i % 10 == 0:  # Periodic cleanup\n",
        "                          torch.cuda.empty_cache()\n",
        "\n",
        "              print(f\"Successfully processed {len(decoded)} frames\")\n",
        "\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"Decoding error: {str(e)}\")\n",
        "              raise\n",
        "\n",
        "        print(\"Displaying Video...\")\n",
        "        display_video(output_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Video generation failed: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        clear_memory()\n",
        "\n",
        "def display_video(video_path):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    mp4 = open(video_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=512 controls autoplay loop>\n",
        "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "print(\"✅ Environment Setup Complete!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rrXFIT4fMfyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Video Generation Parameters\n",
        "# example_prompt = \"A cinematic aerial view from a slowly moving drone, capturing breathtaking landscapes. The camera smoothly glides over rolling green hills, vast forests, and shimmering lakes, bathed in the golden light of sunrise. Mist gently rises from the valleys, creating a dreamy atmosphere. The drone moves gracefully, revealing majestic mountain peaks in the distance, with soft clouds drifting by. Rivers weave through the terrain like silver threads, and vibrant wildflowers dot the fields. The scene is immersive, evoking a sense of wonder and tranquility.\" # @param {\"type\":\"string\"}\n",
        "positive_prompt = \"A drone quickly rises through a bank of morning fog, revealing a pristine alpine lake surrounded by snow-capped mountains. The camera glides forward over the glassy water, capturing perfect reflections of the peaks. As it continues, the perspective shifts to reveal a lone wooden cabin with a curl of smoke from its chimney, nestled among tall pines at the lake's edge. The final shot tracks upward rapidly, transitioning from intimate to epic as the full mountain range comes into view, bathed in the golden light of sunrise breaking through scattered clouds.\" # @param {\"type\":\"string\"}\n",
        "negative_prompt = \"low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly\" # @param {\"type\":\"string\"}\n",
        "width = 832 # @param {\"type\":\"number\"}\n",
        "height = 480 # @param {\"type\":\"number\"}\n",
        "seed = 0 # @param {\"type\":\"integer\"}\n",
        "steps = 25 # @param {\"type\":\"integer\", \"min\":1, \"max\":100}\n",
        "cfg_scale = 2.05 # @param {\"type\":\"number\", \"min\":1, \"max\":20}\n",
        "sampler_name = \"res_multistep\" # @param [\"res_multistep\", \"euler\", \"dpmpp_2m\", \"ddim\", \"lms\"]\n",
        "frames = 73 # @param {\"type\":\"integer\", \"min\":1, \"max\":120}\n",
        "fps = 24 # @param {\"type\":\"integer\", \"min\":1, \"max\":60}\n",
        "\n",
        "with torch.inference_mode():\n",
        "    generate_video(\n",
        "        positive_prompt=positive_prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        seed=seed,\n",
        "        steps=steps,\n",
        "        cfg_scale=cfg_scale,\n",
        "        sampler_name=sampler_name,\n",
        "        length=frames,\n",
        "        fps=fps\n",
        "    )\n",
        "clear_memory()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "roC59_oNNflb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}